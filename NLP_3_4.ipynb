{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+OJN/z79w0sLtWofLjlw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sabirbinsakander/-data-science/blob/main/NLP_3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSwNVx-h3iX6"
      },
      "outputs": [],
      "source": [
        "# import re\n",
        "# text = 'Natural Language Processing is rapidly evolving.'\n",
        "# cleaned_text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "# print(cleaned_text)\n",
        "\n",
        "# #"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLYnMjNS5PlM",
        "outputId": "99d669db-ff9a-4372-86c7-9a254131803f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = wordpunct_tokenize(cleaned_text)\n",
        "print(tokens)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTaLvWUX5vV4",
        "outputId": "4a2502fc-3e6c-47fd-9c2c-dcd3548453c1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'rapidly', 'evolving']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4DCrO359U9",
        "outputId": "78bd2c51-9dce-4b00-f2a1-d4eaa6c6825f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'rapidly', 'evolving']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJd2Hw4367jp",
        "outputId": "8812e7ea-7c54-4b23-9a00-46c77aae0482"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'rapidly', 'evolving']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vector = vectorizer.fit_transform(lemmatized_tokens)\n",
        "print(vector.toarray())\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wosmmhs_70xZ",
        "outputId": "68c9085e-dd35-4dd7-82ca-76ce2e968237"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 0 0 1 0]\n",
            " [0 0 0 0 1]\n",
            " [1 0 0 0 0]]\n",
            "['evolving' 'language' 'natural' 'processing' 'rapidly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"Apple is looking to buy a U.K Startup for $1 billion.\"\n",
        "doc = nlp(cleaned_text)\n",
        "\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWuvTLEzCrNJ",
        "outputId": "ac4f1933-49af-42b8-af10-57a17a026aea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I love this beautiful day, But I am feelign a bit tired\"\n",
        "\n",
        "blob = TextBlob(text)\n",
        "print(f\"Polarity: {blob.sentiment.polarity}\")\n",
        "print(f\"Subjectivity: {blob.sentiment.subjectivity}, Subjectivity: {blob.sentiment.subjectivity}\")"
      ],
      "metadata": {
        "id": "ldfdlWYeDT5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64cd9cc7-7c84-4e9d-c5a1-6a2678fea726"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.1\n",
            "Subjectivity: 0.4, Subjectivity: 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Add this line to download the missing resource\n",
        "\n",
        "text =\"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LM36M20Epk0",
        "outputId": "899f731a-f183-4b18-a70b-b87d48e8b1a3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmcBxCM5GFBh",
        "outputId": "7ec1c197-2371-440d-9b0c-55cc3f52236d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from sumy) (2.32.4)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.12/dist-packages (from breadability>=0.1.20->sumy) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.0.2->sumy) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.0.2->sumy) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.0.2->sumy) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.7.0->sumy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.7.0->sumy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.7.0->sumy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.7.0->sumy) (2026.1.4)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21695 sha256=fbb7c93cd492d6f6d5737ed14225503c33ebf46ff7718d10727fda306cd91ea0\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/99/64/59305409cacd03aa03e7bddf31a9db34b1fa7033bd41972662\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=aae69f432152c7b4698327d1ba510d937bc241ea75b9fa5b29a7a93d58f7c3d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYz8xot8Gf30",
        "outputId": "c41b5824-08b2-4095-e5a0-c55010118b8f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer"
      ],
      "metadata": {
        "id": "rfrotK2AGmvl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"An \"Article for war\" can refer to historical military laws like the Articles of War (US/UK regulations for soldier conduct) or contemporary legal frameworks like the Law of Armed Conflict, governing how wars should be fought to minimize suffering. It can also mean an article about war, covering historical events, personal experiences, the politics of conflict, or the impact of new tech like cyber warfare or autonomous weapons, as seen in publications from the Lieber Institute or news sources.\n",
        "1. Legal & Regulatory Articles\n",
        "Articles of War (Historical): Codified rules for military discipline and conduct, present in U.S. law from colonial times through WWII (replaced by the UCMJ in 1951).\n",
        "Law of Armed Conflict (LOAC): Modern international law (Geneva Conventions, etc.) focused on preventing unnecessary suffering, distinguishing civilians from combatants, and ensuring a return to peace.\n",
        "NATO's Article 5: The collective defense clause, stating an attack on one member is an attack on all, triggering mutual assistance.\n",
        "2. Articles About War (Publications & Themes)\n",
        "Analysis & Commentary: Platforms like Articles of War (Lieber Institute) discuss legal challenges in modern conflict (e.g., cyber warfare, autonomous weapons).\n",
        "Personal Accounts: Essays and articles detailing the human side of war, from soldiers' experiences (snipers, foreign legions) to the effects on civilians.\n",
        "Political/Social Impact: Articles exploring how war reshapes economies, power structures, and international relations (e.g., Ukraine war's effects).\n",
        "Writing Guides: Advice for authors on creating authentic war stories, emphasizing realistic battles and humanized characters.\n",
        "3. Writing About War (Key Elements)\n",
        "Conflict: Multiple types of conflict, not just combat.\n",
        "Realism: Authentic battle scenes.\n",
        "Humanity: Developing relatable characters.\n",
        "Law/Ethics: Applying LOAC principles, even in fiction.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HNq1ksxvGuFG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "summarizer = LexRankSummarizer()\n",
        "summary = summarizer(parser.document, 2)\n",
        "\n",
        "for sentence in summary:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY9C0zUGHLDE",
        "outputId": "08f32404-a439-460a-ffe5-1bfe1cf76c15"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An \"Article for war\" can refer to historical military laws like the Articles of War (US/UK regulations for soldier conduct) or contemporary legal frameworks like the Law of Armed Conflict, governing how wars should be fought to minimize suffering.\n",
            "Personal Accounts: Essays and articles detailing the human side of war, from soldiers' experiences (snipers, foreign legions) to the effects on civilians.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0etg_q0aHbga"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}